{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    " \n",
    "data = []\n",
    " \n",
    "for fileid in brown.fileids():\n",
    "    document = ' '.join(brown.words(fileid))\n",
    "    data.append(document)\n",
    " \n",
    "NO_DOCUMENTS = len(data)\n",
    "print(NO_DOCUMENTS)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bleakiz/Workplace/Projects/LSA/data/Data/10Topics/Ver1.1/Train_Full/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5ad34065cd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bleakiz/Workplace/Projects/LSA/data/Data/10Topics/Ver1.1/Train_Full/'"
     ]
    }
   ],
   "source": [
    "data_path = os.path.dirname(os.getcwd()) + '/data/VNTC/Data/10Topics/Ver1.1/Train_Full/'\n",
    "# data_path = '/home/bleakiz/Workplace/Projects/LSA/data/VNTC/Data/10Topics/Ver1.1/Train_Ful'\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    "\n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in data:\n",
    "    tokenized_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "\n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44931"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at how the 20th document looks like: [(word_id, count), ..\n",
    "len(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI Model:\n",
      "Topic #0: 0.308*\"one\" + 0.280*\"would\" + 0.202*\"said\" + 0.175*\"could\" + 0.146*\"time\" + 0.144*\"new\" + 0.126*\"man\" + 0.125*\"like\" + 0.125*\"two\" + 0.120*\"first\"\n",
      "Topic #1: 0.294*\"said\" + -0.219*\"may\" + -0.179*\"state\" + 0.176*\"could\" + 0.153*\"would\" + -0.143*\"states\" + -0.141*\"new\" + 0.140*\"like\" + 0.139*\"back\" + 0.105*\"man\"\n",
      "Topic #2: 0.340*\"said\" + 0.338*\"state\" + -0.229*\"one\" + 0.191*\"states\" + 0.160*\"year\" + 0.152*\"mrs.\" + 0.135*\"would\" + 0.132*\"united\" + 0.132*\"federal\" + 0.130*\"government\"\n",
      "Topic #3: -0.264*\"new\" + -0.255*\"mrs.\" + 0.155*\"feed\" + 0.152*\"per\" + -0.149*\"world\" + 0.144*\"used\" + -0.139*\"church\" + -0.117*\"god\" + -0.108*\"life\" + 0.101*\"water\"\n",
      "Topic #4: 0.510*\"mrs.\" + -0.235*\"would\" + -0.192*\"states\" + -0.155*\"united\" + -0.131*\"could\" + -0.122*\"man\" + -0.121*\"state\" + -0.108*\"government\" + 0.104*\"year\" + 0.100*\"school\"\n",
      "Topic #5: 0.375*\"feed\" + -0.373*\"would\" + 0.270*\"per\" + 0.243*\"state\" + 0.127*\"god\" + 0.126*\"daily\" + 0.125*\"man\" + 0.120*\"drug\" + 0.116*\"name\" + -0.115*\"school\"\n",
      "Topic #6: -0.276*\"feed\" + 0.262*\"mrs.\" + -0.219*\"per\" + -0.176*\"school\" + -0.168*\"would\" + 0.160*\"states\" + 0.147*\"state\" + -0.139*\"said\" + 0.137*\"united\" + 0.134*\"one\"\n",
      "Topic #7: -0.390*\"mrs.\" + -0.284*\"would\" + 0.258*\"state\" + -0.224*\"feed\" + 0.222*\"said\" + 0.222*\"school\" + -0.143*\"united\" + -0.139*\"per\" + -0.105*\"government\" + 0.103*\"education\"\n",
      "Topic #8: 0.379*\"state\" + 0.274*\"would\" + 0.273*\"mrs.\" + -0.174*\"new\" + -0.161*\"business\" + -0.159*\"united\" + 0.154*\"one\" + -0.128*\"development\" + 0.119*\"feed\" + 0.117*\"federal\"\n",
      "Topic #9: 0.202*\"may\" + -0.192*\"new\" + 0.188*\"mrs.\" + 0.181*\"shall\" + 0.176*\"said\" + 0.163*\"united\" + 0.151*\"school\" + 0.150*\"states\" + -0.148*\"would\" + 0.140*\"form\"\n"
     ]
    }
   ],
   "source": [
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0916149308370161), (1, 0.008747732874593503), (2, -0.016215795883448528), (3, -0.04188890711181653), (4, 0.015237501068488278), (5, 0.011967992144954795), (6, -0.030657832954537664), (7, 0.018542607412777405), (8, -0.0595655422817455), (9, -0.023459283298621177)]\n"
     ]
    }
   ],
   "source": [
    "text = \"The economy is working better than ever\"\n",
    "bow = dictionary.doc2bow(clean_text(text))\n",
    "print(lsi_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['economy', 'working', 'better', 'ever']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(104, 0.8705636), (178, 0.8494539), (31, 0.8456027), (135, 0.8415384), (77, 0.84099627), (85, 0.83988607), (353, 0.83521557), (215, 0.8304162), (254, 0.8264829), (170, 0.82078373)]\n",
      "I am a magazine ; ; my name is Guideposts ; ; this issue that you are reading marks my 15th anniversary . When I came into being , 15 years ago , I had one primary purpose : to help men and women everywhere to know God better , and through knowing Him better to become happier and more effective people . That purpose has never changed . When you read me , you are holding in your hands the product of many minds and hearts . Some of the people who speak through my pages are famous ; ; others unknown . Some work with their hands . Some have walked through pain and sorrow to bring you their message of hope . Some are so filled with gratitude , for the gift of life and the love of God , that their joy spills out on the paper and brightens the lives of thousands whom they have never known , and will never see . Fifteen years ago , there were no Guideposts at all . This month a million Guideposts will circulate all over the world . Experts in the publishing field consider this astounding . The\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    " \n",
    "lsi_index = similarities.MatrixSimilarity(lsi_model[corpus])\n",
    " \n",
    "# Let's perform some queries\n",
    "similarities = lsi_index[lsi_model[bow]]\n",
    "# Sort the similarities\n",
    "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    " \n",
    "# Top most similar documents:\n",
    "print(similarities[:10])\n",
    "# [(104, 0.87591344), (178, 0.86124849), (31, 0.8604598), (77, 0.84932965), (85, 0.84843522), (135, 0.84421808), (215, 0.84184396), (353, 0.84038532), (254, 0.83498049), (13, 0.82832891)]\n",
    " \n",
    "# Let's see what's the most similar document\n",
    "document_id, similarity = similarities[0]\n",
    "print(data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
